---
published: true
layout: single
title: 'Chapter 11. Training Neural Networks'
categories: CV
tag: Pytorch
toc: true
author_profile: false
sidebar:
  nav: 'docs'
---

# Training Neural Networks (Part 2)

- Part 1에서는 학습 전에 고려해야할 사항에 대해 알아보았고, 이번 강의에서는 학습 과정과 학습 후에 고려해야할 사항에 대해 학습 하였다 e.g. Learning rate schedules, transfer learning

## Learning Rate Schedules

- Learning Rate는 SGD, Adagrad, Adam 등 여러 Optimization 기법들의 Hyperparameter임
![1](/assets/image_ch11/1.png)

- 위 그림에서 알 수 있듯이 Learning rate은 학습 효율에 영향을 줌

### Learning rate decay
- 초기에 설정된 Learning rate를 감소시키는 여러 방법이 있음

1. Step
![2](/assets/image_ch11/2.png)
- 정해진 지점마다 일정 수준으로 LR을 줄이는 방법

2. Cosine
![3](/assets/image_ch11/3.png)
- Cosine의 형태롤 LR을 감소시키는 방법

3. Linear
![4](/assets/image_ch11/4.png)
- Linear한 형태로 LR을 감소시키는 방법

4. Inverse sqrt
![5](/assets/image_ch11/5.png)
5. Constant
![6](/assets/image_ch11/6.png)

## Choosing Hyperparameters

### Grid Search

- Hyperparameter로 Grid를 만들어 가능한 모든 경우의 수를 평가하여 고르는 방법

### Random Search

- 일정 범위에서 무작위로 골라 해당 경우에 대해 평가하여 고르는 방법

### Grid vs Random

![7](/assets/image_ch11/7.png)
- 학습을 진행할 때 어떠한 Hyperparameter가 성능에 영향이 큰 것인지 알지 못하기 때문에 Randomness를 활용하는 게 더 나은 선택지임

### Choosing Hyperparameters

- Justin Johnson이 제안하는 GPU가 많지 않을 때의 Choosing Hyperparameters 절차

1. Check initial loss 
: Weight decay 기능을 끄고 loss를 확인해본 뒤, 모델의 오류를 Check

2. Overfit a small sample 
: 작은 size의 Input으로 Training accuracy 100% 달성이 가능한지 Check

3. Find LR that makes loss go down
: Learning rate를 찾는다

4. Coarse grid, train for 1 ~ 5 epochs 
: step 3에서 찾은 values로 1 ~ 5 epochs 정도를 학습시켜본다

5. Refine grid, train longer

6. Look at learning curves

7. Go to Step 5

## Model Ensembles

- 독립적인 여러 모델들을 학습하고 그 결과들의 평균으로 최종 분류를 결정하는 모형

## Transfer Learning

- CNN이나 다른 딥러닝 네트워크를 사용하거나 학습하려고할때 많은 양의 데이터가 없어도 준수한 성능을 낼 수 있는 학습 방법

### Transfer Learning with CNNs

- AlexNet의 네트워크의 마지막 층을 없애고, 학습된 은닉층들을 Feature Extreactor로 활용했을 때, 기존의 Rule based로 설계된 Feature Extractor 보다 우수한 성능을 보임
![8](/assets/image_ch11/8.png)
![9](/assets/image_ch11/9.png)

### Fine-Tuning

- 더 큰 Dataset을 가지고있을 때, 먼저 학습이 완료된 네트워크의 Parameters를 Initial values로 활용하여 학습을 추가로 진행시키는 방법
![10](/assets/image_ch11/10.png)
- ImageNet Classification Challenge에서 좋은 성적을 냈던 모델일수록 Transfer Learning에 적용했을 때 더 좋은 성능을 보임

> ![11](/assets/image_ch11/11.png)
위 표는 Dataset이 ImageNet과 유사한지 유사하지 않은지, Class가 많은지 적은지에 따라 Transfer Learning을 적용하는 방법을 나타냄

## Distributed Training

- GPU의 개수가 많은 환경에서 활용가능한 Training 방법

### Model Parallelism

![12](/assets/image_ch11/12.png)
1. 은닉층을 구간별로 나누어 각 구간을 다른 GPU로 학습시키는 방법
    - 이 방법은 GPU의 대기 시간이 길어지는 단점이 있음
    
![13](/assets/image_ch11/13.png)
2. 모델 자체를 branch들로 나누어 각 branch를 다른 GPU로 학습시키는 방법
    - 각 GPU에서 계산된 Parameters를 합치는 데 비용이 큼
    
### Data Parallelism

![14](/assets/image_ch11/14.png)
- Data를 K개로 나누어 각각의 데이터들로 K개의 GPU에서 학습시키는 방법
- 각 GPU에서 Forward & Backward pass를 진행시킨 후 GPU끼리 통신하여 한꺼번에 갱신하는 방법

### Mixed Model + Data Parallelism

![15](/assets/image_ch11/15.png)
- K개의 Server를 활용하여 각 Server 내에서는 Model parallelism 방식으로 학습을 진행하고 Server끼리 통신하며 갱신을 진행하는 방법
- 그러나, 이 방법은 Minibatches의 size가 매우 커야함

> Large-Batch Training
: GPU의 개수와 Batch size를 키움으로써 학습 시간을 효과적으로 단축시킴


```python

```
