---
layout: single
title: 'Chapter 7. Convolutional Networks'
categories: CV
tag: Pytorch
toc: true
author_profile: false
sidebar:
  nav: 'docs'
---

# Convolutional Networks

- Linear Classifiers는 이미지의 공간적인 구조를 반영할 수 없다는 단점이 있다
- 새로운 Computational nodes를 구상하는 과정에서 제안된 모델구조이다

## Components of a Convolutional Network

- Fully-Connected Layers
- Activation Funciton
- Convolution Layers
- Pooling Layers
- Normalization

## Fully-Conneted Layer

![1](./image_ch7/1.png)

- 위 그림처럼 Input image를 vector로 길게 늘인 뒤, Weight 행렬과 연산을 통해 Output을 계산하는 Layer이다

## Convolution Layer

![2](./image_ch7/2.png)

- Convolution layer는 Input image의 공간적 정보를 활용할 수 있도록 filter를 활용하여 Output인 Activation map을 생성하는 layer이다

- Convolution layer를 일반화하여 나타내면 다음과 같다

![3](./image_ch7/3.png)

- Filter가 있는 Conv layer 또한 Linear한 계산밖에 없기 때문에 비선형적인 요소를 추가해주어야 한다
- Conv layer 다음에 일반적으로 ReLU 함수를 추가하여 비선형성을 추가한다

### Shirink Problem

- 앞서 언급한 Convolution Layer를 적용하게되면 Filter의 크기에 따라 달라지긴 하지만 Activation map이 만들어지게 되면 데이터의 사이즈가 작아진다

- 이때 Padding을 통해 Output의 사이즈를 늘려주게 된다

- 일반적으로 Padding은 Input image의 Pixel 양옆과 위아래에 0을 추가해주는 Zero padding이 사용된다

### Receptive Fields

![4](./image_ch7/4.png)
- 수용영역이라 변역되는 이 개념은 CNN의 작동 방식을 설명해주는 것인데 그림에서 Input에 표시된 Box는 Output으로 계산된 Pixel이 커버하는 수용영역이고 이는 Output의 한개의 Pixel이 Input의 3x3 Pixel을 대변해준다는 뜻이 된다

- 그러나, Input image로 거대한 크기를 가진 것을 사용할 경우, 더 많은 수의 Conv_Layer를 쌓아야 하고 모델이 복잡해지게 된다

- 이때 Stride라는 개념이 도입된다

#### Stride

![5](./image_ch7/5.png)

- Stride는 Filter가 이동하는 pixel의 개수를 늘려서 Output을 Downsapling 해준다

> Convolution Layer를 통과하며 변화하는 Data의 사이즈와 Hyperparameters는 다음과 같다
![6](./image_ch7/6.png)

## Pooling layers

![7](./image_ch7/7.png)
- Stride와 Kernel Size를 조절하는 방법 이외의 Downsample 방법이다
- 그림에 나타난 것은 Max Pooling하는 과정인데 Max이외에 Avg로 Pooling을 진행할 수 있다

> Pooling Summary
![8](./image_ch7/8.png)


## Batch Normalization

- Pooling layer까지 적용한 CNN모델에서 학습이 제대로 되지않는 문제가 발생하였다
- 추측하기로는 SGD Optimizer가 동작할 때 Batch 간 분포가 상이하여 학습과정에 악영향을 주는것으로 생각하였다
- 이를 상쇄하고자 Batch마다 Normalization을 진행하여 분포를 어느정도 조정한다
![9](./image_ch7/9.png)

- 초기에는 Zero-mean과 분산으로 Normalization을 진행하였지만, 경험적으로 문제가 발생하였다
- 이를 해결하고자 학습할 수 있게만든 Parameter인 Gamma와 Beta 항을 Normalization 과정에 추가하였다
![10](./image_ch7/10.png)

### Test-time Problem

- Batch Normalization을 진행할 때 Train-time에서는 Normalization 과정에서 mean과 variance가 Batch size를 이용하여 구할 수 있지만 Test-time에서 한개의 Test data가 Input 되었을 때, Batch Normalization을 할 수 없는 상황이 발생한다
- 이를 해결하기 위해 Test model과 Train model을 다르게 설계하여야 한다
- 또한, Batch Normalization은 선형적인 연산이기 때문에 비선형함수 이전에 추가된다
![11](./image_ch7/11.png)

> 강의가 진행된 시점을 기준으로 아직 Batch Normalization이 이론적으로 확실한 이해가 되고있지 않고 Test모델과 Train모델이 다르게 작동하기때문에 Bug를 일으킬만한 요소가 존재한다.
그러나, 그림에서 볼 수 있듯이 탁월한 성능 향상을 보여주고 있다
![12](./image_ch7/12.png)


```python

```
