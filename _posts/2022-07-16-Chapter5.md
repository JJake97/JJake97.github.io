---
layout: single
title: 'Chapter 5. Neural Networks'
categories: CV
tag: Pytorch
toc: true
author_profile: false
sidebar:
  nav: 'docs'
---

# Chapter5. Neural Networks

- 이번 강의의 내용은 Linear Classifier의 한계를 극복하기위해 도입된 Neural Network Idea에 대한 것이다

## Linear Classifier의 한계

### Geometric Viewpoint
![nn](/assets/image_ch5/1.png)
- 위 그림과 같은 형태의 Dataset에서 분류 성능이 좋은 Decision Boundary를 얻을 수 없다

### Visual Viewpoint
![nn](/assets/image_ch5/2.png)
- Linear Classifier는 class당 하나의 template를 가지고 있어 학습에 사용된 데이터와 class는 동일하지만 형태가 다른 데이터를 구별할 수 없다

### Solution

- Computer Vision 분야에 DL기반 모델이 사용되기 이전에는 Feature Engineering을 통하여 상기 문제에 대한 해결책을 제시하였다

1. Feature transform
![nn](/assets/image_ch5/3.png)
  - 좌표변환을 통하여 Nonlinear classifier를 사용하는 효과를 낸다
  - 이 방법은 Dataset의 모양과 좌표변환의 방법을 알고있어야 유용한 방법이기에 한계가 있다
  
2. Color Histogram
![nn](/assets/image_ch5/4.png)
  - Image에 사용된 Color의 정보를 Histogram으로 나타낸다
  - Input image의 질감과 공간적인 정보는 배제된다

3. HoG(Histrogram of Gradients)
![nn](/assets/image_ch5/5.png)
  - Input image를 8X8로 구역을 나눠 각각의 Gradient를 계산하여 표시한다
  - 색상정보는 무시된다
4. Bag of Words
![nn](/assets/image_ch5/6.png)
  - Data-Driven한 방법으로 Input image를 random한 조각(patch)로 분리하여 추출한다
  - 추출된 patch를 이용하여 Codebook을 만든다
  - Codebook을 만들 때 각 patch에서 해당 patch를 가장 잘 대변하는 Feature를 추출하여 만든다
  - Patch의 Freture를 추출할 때 PCA와 같은 알고리즘이 사용된다
  
- 위와 같은 Feature Engineering을 통해 Input image를 분류하였다
![nn](/assets/image_ch5/7.png)


## Neural Networks

- 앞서 언급한 Linear Classifiers의 한계를 보완하기위해 전통적인 Feature Transforms 방법 이외에 Neural Network 도입을 시도하였다
- 다음은 간단한 2-layer Neural Network를 수식과 그림으로 나타낸 것이다
![nn](/assets/image_ch5/8.png)
- Neural Network방식은 기존의 Linear Classifier 모델에 Nonlinear 성질을 추가하며 Feature Transform과 Classification을 모델 내에서 알아서 수행한다
- 이때, 모든 수행은 Loss를 최소화하는 방향으로 작동하고 Nonlinear 성질을 추가하고자 Activaiton function이 사용된다

### Activation Function

- 비선형적인 성질을 갖게하기 위해 각 Hidden layer에 사용되는 함수로서 일반적으로 ReLU 함수가 사용된다
- Sigmoid, tanh 등 여러 Activation function이 사용되었지만 Gradient를 계산할 때의 문제가 발생하기 때문에 보통 ReLU 함수가 사용된다
![nn](/assets/image_ch5/9.png)

## Space Warping

- Activation Function으로 비선형 함수를 도입하여 분류를 효과적으로 할 수 있는 것에 대한 기하학적인 설명 부분이다

![nn](/assets/image_ch5/10.png)
- 위 그림과 같이 Input data와 Output data가 각각 2차원일 때, 선형변환을 통해서는 효과적인 분류를 수행할 수 없다
- 고차원으로 갈수록 Linear Classifier는 점점 더 분류효과는 떨어지게 된다

![nn](/assets/image_ch5/11.png)
- But, Neural Network에서 비선형 함수인 ReLU 함수를 사용하게되면 단 하나의 직선만으로도 Data가 깔끔하게 분류되는 것을 볼 수 있다
- 일반적으로 Neural Network의 Hidden layer 개수가 많아질수록 높은 차원의 Decision Boundary를 갖게 되지만 Overfitting의 가능성이 생긴다
- Regularization 과정에서 Hidden layer의 개수를 조절하지 않고 Regularization 함수의 세기를 조절해준다

## Universal Approximation

- Neural Network 모델이 어떠한 형태의 함수라도 특정 오차범위 이내에서 근사가 가능하다는 것을 이론적으로 설명하는 부분이다
- 설명에 사용되는 Activation Function은 ReLU이지만, 다른 함수들로도 근사가 가능하다
- Neural Network 내에서 hidden unit들은 계단함수를 생성할 수 있다
![nn](/assets/image_ch5/12.png)
- 생성된 계단함수를 여러개 사용하여 함수를 효과적으로 근사시킬 수 있다
![nn](/assets/image_ch5/13.png)
- 실제 Neural Network가 계단함수를 사용하여 학습이 진행되지는 않지만, 이론적인 설명을 위해 계단함수 개념을 도입하였다


## Convex Functions
- Gradient를 계산할 때, 해당 Weight Data가 Convex Funciton라면 Global Minimum을 쉽게 구할 수 있다
![nn](/assets/image_ch5/14.png)
- Convex Function은 위 수식을 만족하는 함수를 말하는 데, 함수 위의 임의의 두 점 사이를 잇는 직선이 두점 사이의 함숫값들의 집합보다 위에 있어야한다
1. Convex Function의 예시
![nn](/assets/image_ch5/15.png)
2. Convex Function의 조건을 만족하지 않는 예시
![nn](/assets/image_ch5/16.png)

> Global Minimum을 찾는것에 있어서 Convex function이 Powerful하지만 대부분의 Neural network는 Nonconvex optimization을 사용한다
> 그렇기 때문에 많은 Neural network에서 Global minimum을 구할 수 있다라고 하는 경험에 의한 가정을 통해 모델을 설계한다
