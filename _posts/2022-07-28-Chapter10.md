# Training Neural Networks (Part 1)

- 이번 강의에서는 Neural Network를 학습할 때 고려해야하는 사항들에 대해 학습하였다
- 먼저 학습을 진행시키기 전에 고려해야할 사항들에 대해 알아보았다(Activation functions, Data preprocessing, Weight initialization, Regularization)

## Activation Funcitons

- Activation Function은 Input이 노드를 통과하여 선형적인 방법으로 연산된 결과에 비선형성을 부여하는 함수를 뜻한다
- 학습과정에서 Gradient를 계산할 때의 영향을 고려하여 Activation funciton을 선정해야 한다

### Sigmoid

- 0과 1 사이의 값을 Output으로 가지는 함수
- Neural network의 Motivation인 실제 Neuron의 작동 방식(Firing rate)을 잘 설명하기 때문에 역사적으로 많이 쓰이는 함수
![1](./image_ch10/1.png)

#### Sigmoid's Problem

1. 'Kill' the gradients : Input이 극단으로 갈수록 기울기가 너무 작아져서 Gradient가 소실되는 문제

2. Not zero-centered : Output이 항상 양수이기 때문에 역전파 과정에서 Upstream gradient의 부호를 바꿔주지 못하므로 효율적인 학습이 힘들다. 하지만 Minibatches가 이부분을 상쇄시켜준다

3. Exp. 연산이 Bit 연산에서 Expensive 함

### Tanh

- -1과 1사이의 값을 Output으로 가지는 함수
- Zero-centered
- 아직 Gradient가 소실되는문제가 남아있음
![2](./image_ch10/2.png)

### ReLU

- 음수는 0이되고 양의 부분에서는 자신의 값을 가지는 함수
- + region에서 Saturate되지 않는다
- 실제 학습에서 Sigmoid, tanh보다 훨씬 빨리 수렴한다
![3](./image_ch10/3.png)

#### ReLU's Problem

1. Not zero-centered
2. Dead ReLU : 함수의 구조 상 특정 시점에서 Gradient가 0이되어 더 이상 Weight가 update가 안되는 문제가 발생한다. 때문에 0.01정도로 Initialize 해줄 필요가 있다

### Leaky ReLU

- Dead ReLU 문제를 해결하기위해 max함수의 구조를 변경한 함수
- ReLU의 장점을 그대로 가짐
![4](./image_ch10/4.png)

### ELU

- ReLU의 장점을 그대로 가짐
- Leaky ReLU와 비교했을 때 Noise에 대한 Robustness를 더함
- 그러나 Exp() 연산이 필요함
![5](./image_ch10/5.png)

### SELU

- ELU 함수의 변종
- Fully-connected layer만으로 신경망을 구성하고 SELU 함수를 사용하면 네트워크가 Self-normalized되는 함수
- Train하는 동안 각 층의 출력이 평균 0과 표준편차 1을 유지하는 경향을 보인다
![6](./image_ch10/6.png)

## Data Preprocessing

- 데이터 전처리란 학습을 진행하기에 앞서 Input 데이터를 여러 방법으로 가공하여 학습의 효율을 올리거나 모델에 알맞는 형태로 변형시키는 목적으로 수행하는 작업을 의미함
- e.g. Normalize, PCA, Whitening

### 분류 모델에서 Normalization의 효과

![7](./image_ch10/7.png)
- 위 그림처럼 Normalization 전에는 분류 성능이 Weight matrix에 민감하여 최적화에 어려움이 있지만 Normalization 후에는 민감도가 감소하여 최적화가 쉬워짐

## Weight Initialization

- 만약 모든 Weight matrix의 초기값이 0이라면 모든 Output이 0이고 모든 Gradient가 같아짐. 이를 'Symmetry breaking'이라 함

### Small random numbers

![8](./image_ch10/8.png)
- 특정 구간에서 무작위의 값을 선택하는 방법
- small network에서는 잘 작동하지만, Deeper networks에는 문제가 발생함

#### Activation Statistics
![9](./image_ch10/9.png)
- Hidden layer의 size를 4096이라 가정할 때 layer를 지날수록 산포가 줄어들어 0주변의 값들만 선택됨에 따라 Weight initialization의 효과가 사라짐

![10](./image_ch10/10.png)
- 앞의 계수를 0.01에서 0.05로 바꾸어도 양 극단에서는 같은문제가 반복됨

### Xavier Initialization
![11](./image_ch10/11.png)
- sqrt(Din)을 나누어 표준편차를 1/sqrt(Din)으로 만듦
- layer를 지날수록 산포가 줄어들긴 하지만 적당한 산포를 유지하는것을 볼 수 있음
- 몇가지 가정을 통해 단순화하여 해당 수식을 도출함

#### Xavier initialization Problem

- Xavier initialization은 zero-centered를 가정하고 있기 때문에 Activation function을 지날 때 0주변으로 분산이 줄어드는 문제가 다시 발생함
![12](./image_ch10/12.png)
- 히스토그램 상으로 볼때 분산의 절반이 날아간 상황이기 때문에 수식을 sqrt(2/Din)으로 수정함(MSRA Initialization)
- 수식 수정 후, 추가적으로 이를 ResNet에 적용 시에 Residual Block을 지날 때마다 분산이 증가하는 문제가 발생함
- Residual Block의 첫번째 Conv_layer에는 MSRA을 적용하고 두번째 Conv_layer에는 Weight를 0으로 조정하여 분산을 같게 만들어줌


## Regularization

- Overfitting 문제를 해결하기 위해 모델에 규제를 거는 방법
- L2, L1 Regularization이 일반적인 상황에서 사용됨

### Dropout
![13](./image_ch10/13.png)
- Forward pass 과정에서 일정 확률에 의해 Node를 버리는 방법
- Test time에서는 Dropout에 의해 Output이 Random하게 나오므로 이러한 randomness없애기 위해 확률에 의한 결과값을 근사하여 단순하게 Dropout하는 확률을 곱해준다
- 일반적으로 학습해야하는 Parameter가 많은 Fully-connected layer에 많이 사용되며 최근의 모델에는 Fully-conneted layer가 잘 사용되지 않기 때문에 최근에는 사용 빈도가 떨어지는 방법임

### Data Augmentation

- Input Image에 변화를 주어 모델에 규제를 가하는 방법 e.g. Random Crops and Scales, Horizontal Flips, Color Jitter

### DropConnect
![14](./image_ch10/14.png)
- Node를 버리는 것이 아닌 노드들간의 연결을 끊어버리는 Regularization 방법

### Fractional Pooling

- Pooling region을 무작위로 적용하는 방법

### Stochastic Depth

- ResNet에서 몇개의 Residual block을 건너뛰는 방법

### Mix up
![15](./image_ch10/15.png)
- Input image들을 무작위로 혼합하는 방법

> Size가 큰 Fully-connected layers를 사용하는 모델에는 Dropout 사용을 고려해볼 수 있음

> Batch normalization과 Data augmentation은 대부분의 경우에 잘 작동하는 방식임
