---
layout: single
title: 'Chapter 12. Recurrent Neural Networks'
categories: CV
tag: Pytorch
toc: true
author_profile: false
sidebar:
  nav: 'docs'
---

# Recurrent Neural Networks

- 이번 강의에서는 앞서 배웠던 "Feedforward"방식의 Neural Networks가 아닌 Sequence data에 대한 학습이 가능한 Recurrent Neural Networks에 대해 학습하였다.

## Process Sequences

- Input과 Output data에 따라 구분되며,크게 4가지 형태가 있다
1. One to One
2. One to many
3. Many to one
4. Many to many

![1](/assets/image_ch12/1.png)

- 또한, Non-sequential data를 sequencial한 형태로 입력을 받아 Sequential Processing 가능하다

## RNN

![2](/assets/image_ch12/2.png)
- 기본적인 RNN의 구조는 위 그림과 같으며, hidden state를 계산할 때 사용되는 Weight은 모델 전체에서 동일하다.
- 현 시점의 hidden state를 계산할 때 이전 시점의 hidden state와 현 시점의 input data가 연산에 사용된다

### Vanilla RNN

- Activation function으로 tanh를 쓴 RNN의 기본모델

#### Language Modeling with Vanilla RNN

- Vanilla RNN모델을 단어 예측에 사용한 예시 
- 주어진 "hello"를 통해 각 시점의 charcter를 예측

![3](/assets/image_ch12/3.png)

- input char 'h'를 통해 연산된 output 데이터로 'e'를 예측하는 방식
- 이후, 예측 된 'e'를 one-hot-encoding된 값을 다음 시점의 input으로 사용
- 그러나 Gradient를 계산할 때, 전체 Sequence를 봐야하기 때문에 memory가 많이 사용된다
- 이를 해결하기 위해 전체 Sequence를 쪼개어 Backpropagation을 진행한다

### Vanilla RNN Gradient Flow

![4](/assets/image_ch12/4.png)

- 위 그림은 Vanilla RNN의 Gradient가 계산되는 흐름을 나타낸 것이다
- Forward pass에서 hidden state를 계산할 때 Weight가 곱해지게되는데, Backprop 과정에서 초기 h0의 gradient에는 앞부분의 hidden state에서 이어져온 Weight가 계속해서 곱해지게 된다
- 이로인해 Exploding gradients, Vanishing gradients 문제가 발생할 수 있다

- Exploding gradients문제는 gradient의 Scale을 조정함으로써 해결이 가능하지만, Vanishing gradient는 RNN의 구조를 변경하여야 한다

### LSTM (Long Short Term Memory)

- 기존의 RNN 모델은 예측이나 분류를 수행할 때, 필요한 정보의 시간적 격차가 크지 않다면 우수한 성는을 보여주었다. 하지만, 시간적 격차가 큰 경우에 대해서는 좋은 성능을 보이지 못한다.
- LSTM은 이부분을위해 고안된 모델로써, long-term dependencies를 효과적으로 다룬다

![5](/assets/image_ch12/5.png)

- LSTM 모듈의 구조는 위 그림과 같다.
- LSTM의 핵심은 Cell state라고 불리는 것이다. 해당 state의 정보를 유지하거나 더하거나 없앨 수 있으며, 이 것은 Gate라는 구조에 의해 제어된다

1. Forget gate layer
![6](/assets/image_ch12/6.png)

- Cell state로부터 어떤 정보를 버릴 것인지 정하는 layer
- Sigmoid function을 사용하며 0과 1사이 값을 보내어 정보 보존의 정도를 결정한다

2. Input gate layer
![7](/assets/image_ch12/7.png)

- 어떤 정보를 Cell state에 저장할 것인가를 정하는 layer
- input gate layer에서 sigmoid function에 의해 어떤 값을 업데이트할 것인지를 정한 다음, tanh layer에서 새로운 후보 값들의 vector를 생성하여 곱한 결과를 Cell state에 더한다
- 이때, 더하게 되는 Cell state는 forget gate layer에 의해 특정 정보가 버려진 Cell state이다

3. Output gate layer
![8](/assets/image_ch12/8.png)

- 무엇을 Hidden state의 Output으로 내보낼지를 정하는 layer
- sigmoid function에 input 데이터를 넣어 나온 값과 새로 업데이트 된 Cell state에 tanh를 씌운 값을 곱하여 Hidden state를 결정한다

> gate layer들과 Cell state를 추가함으로써 아래 그림과 같은 Gradient flow를 얻을 수 있다.
![9](/assets/image_ch12/9.png)
