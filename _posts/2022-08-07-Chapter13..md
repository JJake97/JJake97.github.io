# Chapter13. Attention

- 이번 강의에서는 기계번역 분야에 자주 사용되는 Attention 모델에 대해 학습하였다

## Sequence to Sequence with RNNs

![1](./image_ch13/1.png)
- 기존의 RNN모델로 Seq 2 Seq 학습의 예시인 기계번역을 수행할 때 Encoder를 통해 계산된 input 문장의 정보를 Context vector의 형태로 받아온다

- 이후, 초기 Decoder state s0와 초기 Output y0, Input으로부터 받아온 context vector를 가지고 새로운 s1을 만들게 되고 s1과 y0을 통해 새로운 Output y1을 얻는다

- y1은 다음 예측의 Input으로 들어가 s2 계산에 사용된다

### Problem

- Context vector는 일반적으로 마지막 Hidden state를 사용하는데, 비교적 최근의 정보의 영향을 더 많이 받는 RNN 모델의 특성 상 긴 문장을 번역해야할 경우, 성능이 떨어질 수 있다

## Attention

- Decoder에서의 각 step마다 새로운 context vector를 사용하는 모델

- 새로운 context vector는 전체 input 문장을 한번 더 참고하여 생성되며 이때 동일한 비율로 참고하는 것이 아니라 해당 시점에서 예측해야할 단어와 연관이 있는 부분을 좀더 집중해서 보게 된다

## Attention의 구조
![2](./image_ch13/2.png)

- 기존의 RNN구조에서 alignment scores 개념을 도입하여 이전 시점의 Decoder state Encoder의 hidden state들을 사용하여 alignment score를 계산한다
- 이를 softmax 함수를 통해 확률값처럼 변환을 시키고, 다시 hidden states를 곱하고 이를 더해줌으로 써 새로운 context vector c1을 만든다
- c1과 y0과 s0을 이용하여 s1을 계산하고, s1과 y0을 y1 예측에 사용한다

> Decoder의 time step마다 context vector를 새롭게 계산함으로인해 하나의 context vector를 사용함으로써 발생하는 정보 소실 가능성을 없앨 수 있고, 매 timestep마다 input sequence의 다른 부분을 볼 수 있는 효과를 가진다

> 또한, 기계번역에 Attention 모델을 사용할 때 Input 정보의 순서가 Decoder의 계산에 반영되지 않기 때문에 여러가지 분야로 Attemtion모델의 사용처를 확장해볼 수 있을 것이다 e.g. Image captioning, Audio region, Robot control

## Attention layer

- 기계번역 분야에서 사용되던 Attention 개념이 Image, Audio, Robot control 등 다양한 분야로 확대됨에 따라 Attention 개념을 추상화하여 Attention layer를 정의한다

![3](./image_ch13/3.png)

- 프로그래밍의 Dictionary 자료형의 key-value 개념을 차용하여 Key Vectors, Value vectors를 새롭게 정의한다. 이때 두 vectors 모두 X를 활용하여 계산한다.

- Key vectors는 Query vectors와의 연산을 통해 Similarites를 계산하고, Value vectors는 Similarities에 softmax를 취한 Attention weights와의 연산으로 Output vectors를 계산한다

- 이렇게 추상화를 하였지만, 각 time step마다 Encoder의 정보를 참고하는 Attention의 기본적인 개념은 변하지 않았다

## Self-Attention layer

![4](./image_ch13/4.png)

- 앞서 언급한 attention layer는 연산에 필요한 Query vectors가 이미 만들어져 있어야한다.
- 하지만, self-attention layer는 Input vectors를 통해 Query vectors 연산을 진행한다.
- Input vectors를 통해 Query vectors를 생성함으로써 Alignment matrix 연산시에 자기 자신을 한번 더 참고하는 결과를 얻는다

- Self-attention layer는 Input 단어 순서를 기억하지 않는다. 예를 들어, 어순이 이상한 문장 즉, 문법적인 오류가있는 문장이 Input으로 사용되어도 Output으로 해당 Input 각각의 단어에 대응되는 Output vectors를 결과로 산출한다. 
    - 만약 단어 순서를 기억했다면, 훈련 데이터에 없을법한 이상한 배열의 문장이 주어졌을 때 엉뚱한 결과값을 도출할 것이다.
    - Input 단어의 순서를 기억하게 만들기 위해서는 Input vectors에 위치를 기록해놓는 positional encoding을 추가하면 된다.

### Masked self-attention layer

![5](./image_ch13/5.png)

- 예측 분야에 self-attention을 사용하기 위해서는 예측하고자하는 위치보다 미래의 시점의 Input vectors는 연산에 사용되서는 안된다.
- 이를 해결하기위해 이후 시점의 Attention matrix에 Mask를 씌우는 방법이 개발되었다.


## Transformer

![6](./image_ch13/6.png)

- Seq-to-Seq 문제에 사용되는 Encoder-Decoder의 구조를 따르면서도, Attention구조 만으로 구현한 모델

- RNN을 사용하지않고, Encoder-Decoder 구조를 설계했음에도 RNN보다 우수한 성능을 보인다

- Transformer에서는 RNN구조를 사용하지 않기 때문에, 각 단어의 위치를 기억하게하는 작업이 필요하다. 이를위해 Positional Encoding을 진행한다.

- 또한, Self attention layer를 지난 다음 residual connection을 통해 Input의 정보를 더해준다

- 위 그림과 같은 Transformer block을 여러 개 쌓아서 Transformer 모델을 만든다
