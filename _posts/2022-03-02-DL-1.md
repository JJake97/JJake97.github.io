---
layout: single
title: 'Deep Learning #1'
categories: DL
tag: DL
toc: true
author_profile: false
sidebar:
  nav: 'docs'
---

# 인공신경망

- 컴퓨터가 인간처럼 학습하는 것을 구현하기 위해 고안해낸 개념
- 수많은 인공뉴런들을 서로 그물망처럼 묶어서 정보를 앞에서 뒤로 전달
- 출력층에서 나온 값을 가지고 예측을 시행한다

## 인공뉴런

- 인간의 뉴런과 비슷한 기능을 하는 인공적인 뉴런
- 여러 숫자들을 받아서 하나의 숫자를 리턴하는 함수
- 가중치(Weight), 편향(Bias)를 활성함수에 입력변수로 넣어 계산한다

### 가중치, 편향

- 가중치
  - 전층 뉴런의 출력에 곱해지는 값
  - 어떤 층에 있는지, 어떤 뉴런과 어떠한 순서 연결되는지에 따라 구분된다

- 편향
  - 전층 뉴런의 출력과 가중치의 곱들과 함께 더해지는 값

### 순전파

- 입력층으로 들어오는 정보가 층 단위로 마지막 층까지 계산되는 과정
- 전 층 모든 뉴런의 출력과 가중치를 곱한 후, 편향과 함께 더한 값을 계산하고 그 계산값을 활성함수에 넣는다

### 역전파

- 인공신경망은 매우 복잡한 합성함수로 이루어져 있기 때문에 일반적인 경사하강법을 적용하기 어렵다
- 이와 같은 인공신경망에서 경사 하강법을 구현하기위해 역전파 알고리즘을 고안했다
- 역전파 알고리즘은 각 가중치와 편향에 대한 손실 함수의 편미분, 즉 손실함수가 어떻게 바뀌는지 계산하는 과정에 쓰이는 알고리즘이다
- 뒤 층들에서 계산한 편미분 값들을 앞 층 요소들의 편미분을 계산할 때 전달해줌으로써 각각의 가중치와 편향이 출력에 미치는 영향을 고려하여 편미분을 계산해준다

### 인공신경망의 비선형성

- 매우 복잡한 형태의 분류가 필요한 데이터에서는 로지스틱 회귀모델과 같은 선형적인 모델은 좋은 성능을 내지 못한다
- 인공신경망을 사용하여 해당 데이터를 학습하면 신경망의 비선형성을 이용하여 보다 복잡한 분류를 가능하게 해준다
- 이러한 비선형성은 은닉층의 활성함수를 통해 이뤄진다

### 은닉층 활성함수

- 시그모이드 함수
    - 입력변수를 0과 1사이의 숫자로 바꿔주는 함수이다
    - 시그모이드 함수를 활성함수로 사용하면 Vanishing gradient problem이 생긴다
    - 이는 가중치와 편향에 대한 편미분 값들이 0에 수렴하는 문제이다
- ReLU
    - Vanishing gradient problem을 개선한 함수
    - 아웃풋이 0보다 크면 그대로, 0보다 작으면 0을 반환하는 함수이다

### 출력층 활성함수

- 시그모이드 함수
    - 이분적 분류 문제에서 주로 활용되는 출력층 활성 함수이다
    - 마지막 출력층에 활성함수로 선형함수를 사용하여도 Vanishing gradient problem이 생기지 않기 때문에 시그모이드 함수를 사용하여도 된다
- Softmax 함수
    - 다중 분류 문제를 해결할 때 사용된다
    - 다중 분류 문제에서 시그모이드 함수를 사용하면 마지막 층 뉴런들의 출력의 합이 1을 넘어갈 수 있기때문에 확률적인 해석을 할 수없다, 이를 보완하기위해 Softmax 함수는 마지막 층 뉴런들의 출력의 합을 1로 바꿔준다
- 선형 함수
    - 연속적인 값을 맞추는 회귀 문제를 해결할 때 사용된다

### 신경망 손실 함수

- 다양한 손실 함수를 사용하지만, 대표적인 손실함수로는 평균 제곱 오차, 크로스 엔트로피등이 있다

### 신경망 정규화

- 신경망은 매우 복잡한 구조를 가지고 있기 때문에 정규화를 진행하지않으면 과적합의 문제에 빠지게 된다
- 이를 해결하기 위한 정규화 방법을 대표적인 것이 L1 정규화, L2 정규화가 있다
