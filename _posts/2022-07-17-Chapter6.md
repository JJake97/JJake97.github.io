# Chapter6. Backpropagation

- 경사하강법을 위한 Gradient를 구하는 역전파 알고리즘에 관한 내용이다
- Loss 최적화를 위해 Gradient를 구하는 과정에서 무수한 행렬 연산을 수행하기에는 계산이 오래걸리며 비효율적이다
- 또한, 모델마다 사용하는 Loss function의 종류가 다르다면 그에 따른 도함수도 달라지기 때문에 계산방식이 달라진다
- AlexNet과 같은 복잡한 구조를 가진 모델의 Gradient를 연쇄법칙의 성질을 활용한 Backpropagation 알고리즘을 통해 쉽게 구할 수 있다


  > AlexNet의 구조
  ![nn](./image_ch6/1.png)
  
  > 복잡한 구조의 함수를 노드별로 분리하여 그래프를 그린 후, 반대방향으로 차근차근 Gradient를 계산한다
  ![nn](./image_ch6/2.png)

## Simple Example
![nn](./image_ch6/3.png)
- f(x,y,z) = (x + y)z 라는 간단한 함수를 통해 역전파 알고리즘을 알아보고자 한다.
- 이때, x = -2, y = 5, z = -4

> Forward pass에서 각 +와 * 노드의 함숫값을 구하고 Backward pass에서 각 element의 편미분값을 계산한다

- 전체적인 Backpropagation을 그림으로 나타내면 다음과 같다
![nn](./image_ch6/4.png)

  - Downstream gradients은 노드를 거쳐간 계산값의 gradient인 Upstream gradient와 해당 노드에서의 gradient인 Local gradient에 의해 계산된다
  - 이러한 방식을 사용하면 복잡한 함수의 gradient도 간편하게 계산이 가능하다

## Patterns in Gradient Flow
- Graph로 나타낸 노드 유형별로 Gradient가 계산되는 패턴이 존재한다

1. Add
  - 더하기 연산에 미분을 취하면 1이 되기 때문에 Upstream gradient를 그대로 취한다
![add](./image_ch6/6.png)

2. Mul
  - 곱하기 연산에 미분을 취하면 상수 취급된 변수만 남기 때문에 "swap multiplier"라고 불린다
  ![add](./image_ch6/7.png)
  
3. Copy
  - 2차원으로 변환하는 행렬연산에서 각각의 element를 미분하면 1이 되기 때문에 Upstream gradient의 element를 더한 값을 취한다
  ![add](./image_ch6/8.png)
  
4. Max
  - 노드를 통해 Max 값으로 선택된 Input의 Downstream gradient는 Upstream gradient를 그대로 취하고 선택이 안된 Input의 gradient는 0이 된다
  ![add](./image_ch6/9.png)

> 이미 Pythorch, Tensorflow등의 딥러닝 Framework들에서 주로 쓰이는 함수들의 Gradient 계산에 필요한 정보들을 Module 형태로 만들어서 제공하고있다

> 예시에서 보인것과 같이 Input과 Output이 상수형태가 아닌 Vector와 Matrix형태에 대해서도 같은 방식으로 Backpropagation을 적용할 수 있다
> 해당 역전파 알고리즘에 관한 강의를 요약하자면
    1. 복잡한 함수형태를 computational graph형태로 만들어 역방향으로의 계산을 통해 전체 Gradient를 구한다
    2. Backpropagation을 위해 Upstream, Local, Downstream gradient 값이 사용되며 Downstream은 편미분의 연쇄법칙에 따라 Upstream과 Local gradient를 이용하여 계산한다
