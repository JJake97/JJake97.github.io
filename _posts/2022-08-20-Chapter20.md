# Generative Models II

## More Variational Autoencoders

![1](./image_ch20/1.png)

- VAE는 variational lower bound를 최대화하는 방향으로 학습이 진행됨

![2](./image_ch20/2.png)

- 위 그림과 같이 Encoder에 의해 Latent code를 생성한 다음

![3](./image_ch20/3.png)

- 위 그림과 같이 Decoder에 의해 data의 분포를 추정하고 그 분포내에서 sampling을 통해 reconstructed data를 생성함
- 해당 reconstructed data는 Input data와 유사함

- 학습이 완료된 VAE를 이용하여 새로운 data를 생성할 수 있으며, 이미지를 수정하여 생성할 수 있음

![4](./image_ch20/4.png)

- 위 그림은 VAE를 통해 생성된 데이터이고,

![5](./image_ch20/5.png)

- 위 그림은 VAE를 통해 일부분 수정된 이미지 데이터임

## Generative Adversarial Networks

- 적대적 생성 신경망이라고 불리는 모델
- Generator와 Discriminator 두개의 네트워크로 이루어진 모델

1. Generator : 하나의 latent vector z를 입력으로 받아서 새로운 데이터 instance를 생성
2. Discriminator : 해당 생성 instance를 입력으로 받아서 이것이 학습데이터의 분포에서 나온것인지 확률값으로 나타냄

![6](./image_ch20/6.png)

- 수식으로 나타내면 위 그림과 같음
- Discriminator는 어떻게 해서든 objective function을 최대화하는 방향으로 학습할 것이고, Generator는 objective function을 최소화하는 방향으로 학습시킬 것임

- 그러나 실제 훈련 진행 시, Descriminator와 Generator가 각각의 손실을 가지고 있기 때문에 두가지 손실이 상호작용하며 학습이 제대로 진행되지 않았음

- 또한, 학습 초기에 생성자의 성능이 너무 낮아 반대 급부로 판별자의 성능이 너무 높아지기 때문에 초기 D(G(z))의 gradient가 0에 매우 가까워지는 문제가 있음

![7](./image_ch20/7.png)

- 해당 문제를 해결하기 위해 식을 조금 수정하여 아래 그림에서 주황색을 나타내는 수식으로 학습을 진행하였고, 그 결과 학습 초기에 좋은 gradient를 얻어냄

![8](./image_ch20/8.png)

### Results

![9](./image_ch20/9.png)

### DC-GAN

- 앞서 학습한 GAN은 MNIST와 같은 간단한 이미지 생성에는 좋은 성능을 보이나 복잡한 이미지를 생성하는데 한계점이 있었음
- DC_GAN은 Generator 부분을 구성할 때, 이미지에 특화된 모델인 CNN을 적용한 모델임

![10](./image_ch20/10.png)

### GAN Improvements

- 2016년 이후, GANs을 발전시킨 논문들이 폭발하듯이 쏟아졌고, 몇몇의 논문들은 괄목할만한 발전을 보였음

#### Improved Loss Functions

- Loss function을 수정하여서 성능 향상을 가져온 모델들

1. Wasserstein GAN
2. WGAN with Gradient Penalty

> 이후 Conditional GANs, Pix2Pix, CycleGAN 등 다양한 GAN 모델이 연구되고 있고, 현재까지도 새로운 형태의 GANs이 쏟아지고 있음
