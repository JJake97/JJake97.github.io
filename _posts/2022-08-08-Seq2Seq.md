## Seq2Seq 기계 번역 모델 실습

- Sequence 데이터를 입력받아 또 다른 Sequence Output을 출력하는 모델
- 기계 번역을 제대로 구현하려면 원문과 번역문이 쌍을 이루는 형태의 방대한 데이터가 필요하다
- 그러나 이번 실습에는 간소화하여 영어 알파벳 문자열 'hello'를 스페인어 알파벳 문자열 'hola'로 번역하는 모델을 구현해보도록 한다

#### 필요한 라이브러리 Import


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import matplotlib.pyplot as plt
```

#### 배열 정의 및 Tensor 변환

- 훈련 데이터로 입력받을 원문과 번역문을 아스키 코드의 배열로 정의하고 Tensor로 변환해줍니다

> 여기서 아스키코드는 미국정보교환표준부호로 영어 알파벳을 숫자로 표현할 때 사용됩니다


```python
vocab_size = 256  # 총 아스키 코드 개수
x_ = list(map(ord, "hello"))  # 아스키 코드 리스트로 변환
y_ = list(map(ord, "hola"))   # 아스키 코드 리스트로 변환
print("hello -> ", x_)
print("hola  -> ", y_)
```

    hello ->  [104, 101, 108, 108, 111]
    hola  ->  [104, 111, 108, 97]



```python
x = torch.LongTensor(x_)
y = torch.LongTensor(y_)
```

#### 모델 클래스 정의, Embedding 함수 정의


```python
class Seq2Seq(nn.Module):
    def __init__(self, vocab_size, hidden_size):
        super(Seq2Seq, self).__init__()
        self.n_layers = 1
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.encoder = nn.GRU(hidden_size, hidden_size)
        self.decoder = nn.GRU(hidden_size, hidden_size)
        self.project = nn.Linear(hidden_size, vocab_size)

    def forward(self, inputs, targets):
        # 인코더에 들어갈 입력
        initial_state = self._init_state()
        embedding = self.embedding(inputs).unsqueeze(1)
        # embedding = [seq_len, batch_size, embedding_size]
        
        # 인코더 (Encoder)
        encoder_output, encoder_state = self.encoder(embedding, initial_state)
        # encoder_output = [seq_len, batch_size, hidden_size]
        # encoder_state  = [n_layers, seq_len, hidden_size]

        # 디코더에 들어갈 입력
        decoder_state = encoder_state
        decoder_input = torch.LongTensor([0])
        
        # 디코더 (Decoder)
        outputs = []
        
        for i in range(targets.size()[0]):
            decoder_input = self.embedding(decoder_input).unsqueeze(1)
            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)
            projection = self.project(decoder_output)
            outputs.append(projection)
            
            #티처 포싱(Teacher Forcing) 사용
            decoder_input = torch.LongTensor([targets[i]])

        outputs = torch.stack(outputs).squeeze()
        return outputs
    
    def _init_state(self, batch_size=1):
        weight = next(self.parameters()).data
        return weight.new(self.n_layers, batch_size, self.hidden_size).zero_()
```

#### CrossEntropyLoss 클래스와 최적화 알고리즘 정의


```python
seq2seq = Seq2Seq(vocab_size, 16)
```


```python
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(seq2seq.parameters(), lr=1e-3)
```

#### 학습 진행


```python
log = []
for i in range(1000):
    prediction = seq2seq(x, y)
    loss = criterion(prediction, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_val = loss.data
    log.append(loss_val)
    if i % 100 == 0:
        print("\n 반복:%d 오차: %s" % (i, loss_val.item()))
        _, top1 = prediction.data.topk(1, 1)
        print([chr(c) for c in top1.squeeze().numpy().tolist()])
```

    
     반복:0 오차: 5.660667419433594
    ['{', '{', 'D', 'D']
    
     반복:100 오차: 2.0454797744750977
    ['h', 'o', 'l', 'a']
    
     반복:200 오차: 0.530512809753418
    ['h', 'o', 'l', 'a']
    
     반복:300 오차: 0.24105407297611237
    ['h', 'o', 'l', 'a']
    
     반복:400 오차: 0.13426250219345093
    ['h', 'o', 'l', 'a']
    
     반복:500 오차: 0.088699571788311
    ['h', 'o', 'l', 'a']
    
     반복:600 오차: 0.06452015042304993
    ['h', 'o', 'l', 'a']
    
     반복:700 오차: 0.049704428762197495
    ['h', 'o', 'l', 'a']
    
     반복:800 오차: 0.0397779680788517
    ['h', 'o', 'l', 'a']
    
     반복:900 오차: 0.03271286562085152
    ['h', 'o', 'l', 'a']



```python
plt.plot(log)
plt.ylabel('cross entropy loss')
plt.show()
```


    
![png](output_13_0.png)
    



```python

```
