# Chaper3. Linear Classifiers

![nn](image/1.png)

- Neural Network는 사진과 같이 여러개의 Linear Classifier로 구성된다
- Linear Classifier는 Computer Vision에서 단순하지만 핵심적인 구성요소이다

## Parametric Approach

![nn](image/2.png)

- Linear Classifier는 x(input image)와 W(weight)가 분류에 사용된다
- 주어진 input image를 하나의 column vector로 변환한 뒤, Weight 행렬과의 곱 연산을 통해 각 Category의 Score를 계산한다
  - 이때 각 Category란 Dataset에서의 각 label을 의미하고 수업에 활용된 CIFAR10의 경우 10개의 Category가 존재한다

## Algebraic Viewpoint

![nn](image/3.png)

- 대수적인 관점에서 Linear Classifier는 행렬(Weight)와 벡터(input image)의 곱 연산에 벡터(bias)를 더한것으로 이해된다
- 위의 그림에서 bias를 따로 더해주지않고 W 행렬에 추가하여 계산과정을 줄일 수 있다
- 연산의 결과로 도출되는 각 Category별 Score는 Linear한 성질을 가지며, 이로인해 달라질 수 있는 계산결과는 손실함수(loss function)을 활용하여 대처할 수 있다

## Visual Viewpoint
![nn](image/4.png)
- 특정 Dataset으로 학습된 Linear Classifer는 각 Category별로 "Template"을 가진다
  - 예를 들면 새로운 image를 분류할 때 가운데 파란색 물체가 있으며 배경 또한 파란색이라면 Plane Cetegory의 Score가 높게 계산될 것이다
  
- Linear Classifer는 모델별로 한개의 Template을 가지기 때문에 Data의 Mode를 구별할 수 없다
- 또한 학습에 사용된 Dataset에 의존성이 높기때문에 같은 Category이지만 전혀 다른 형태의 image를 분류하는 성능은 떨어진다

## Geometric Viewpoint

![nn](image/5.png)

- 기하적인 관점에서 Linear Classifer는 고차원의 유클리드 공간을 각 Category의 Hyperplane이 2등분하여 각각의 이미지를 분류하고 있다
- 이러한 기하학적인 관점은 차원의 저주 문제로 인해 상상하기 매우 힘든 관점이지만 고차원의 연산을 이해하는 데 효과적이다

## Hard cases for a Linear Classfier

- Linear Classifer는 2차원 공간상에서 보면 1개의 직선이 해당 이미지를 분류하는 것이기 때문에 분류가 어려운 Case가 존재한다

![nn](image/6.png)
- 첫번째 그림에서는 1,3 사분면과 2,4 사분면을 직선하나로 나눌 수 없기 때문에 분류가 힘든 Case에 속한다
- 두번째 그림은 원점을 중심으로 원 형태를 가지고 있기 때문에 분류가 힘들다
- 세번째 그림 또한 특정 크기의 점들이 무작위로 존재하기 때문에 한개의 직선만으로는 정확한 분류를 수행하기 어렵다

## Loss Function

- Linear Classifier의 연산 결과에서 도출되는 각 Category별 Score를 통해 W 행렬이 Good Value인지 판단하기 위해 Loss Function을 도입한다
- Loss와 Classfier의 성능은 반비례 관계를 가진다
- Loss는 항상 작아지는 방향으로 학습이 진행된다

![nn](image/7.png)
- 위 수식은 N개의 Category를 가진 데이터셋에서 x는 image, y는 label을 의미한다
  
- 특정 Loss Function을 사용하여 Loss 계산 수식은 다음과 같다
![nn](image/8.png)

### Multiclass SVM Loss

![nn](image/9.png)

- Correct Class의 Score가 다른 Score들보다 높을 것이라는 가정에서 출발하는 Loss Function이다
- 다른 Score와 Correct Class의 Score의 차이 더하기 Margin이 0보다 클 경우 해당 값을 반환하고 반대의 경우 0을 반환한다
- 보통의 경우 Margin은 1로 설정한다
- Class score가 조금 변경되어도 그 차이가 Margin보다 작다면 loss는 바뀌지 않는다
- 해당 함수에서 Loss의 최댓값은 무한대, 최솟값은 0이다
- 만약 각 class의 score가 random값을 취할 경우, score가 가우스 분포를 따른다면 각 score들의 차이는 0에 가깝게 되므로 Loss는 (C-1)Margin이 된다
  - 이를 통해 모델을 Debuging할 수 있다 모델의 Loss가 (C-1)Margin을 크게 벗어난다면 어딘가에 Bug가 있다는 뜻이기 때문이다

### Cross-Entropy Loss(Multinomial Logistic Regression)
![nn](image/10.png)

- Cross-Entropy Loss는 raw classifier score를 확률로 활용하기 위한 함수이다 이를 위해 Softmax Function을 사용한다
- Softmax Function에 의해 각 Score들이 확률값으로 변환되며 각각의 확률값에 -log를 취하여 Loss를 계산한다

#### Cross-Entropy
- Cross-Entropy는 두 개의 확률분포 p,q에 대해 하나의 사건이 가지는 정보량으로 정의된다.
- 이때 Cross-Entropy를 최소화하는 것이 KL Divergence를 최소화하는 것과 같기 때문에 이 개념이 Loss Function에 활용된 것이다


## Regularization
![nn](image/11.png)

- Regularization을 도입하게 된 목적은 Overfitting을 방지하기 위함이다
- Overfitting이 되면 학습 Dataset에서는 성능이 우수하지만 새로운 Data에 대한 성능은 현저하게 떨어진다
- 이를 위해 L1, L2, Dropout등 다양한 방법이 사용된다
- Loss Function의 수식을 바꾸어 W를 조절하는 방향으로 Regularization이 이루어지게 된다
- 결과적으로 모델을 단순하게 만드는 방향으로 Regularization이 이루어진다

### L1 Regularization
- Loss Function 수식에 Weight의 절댓값을 더하는 수식의 형태로 이후에 있을 Optimization 과정에서 Weight에 대한 편미분을 수행했을 때 상수값을 빼주는 형태가 된다
- 이를 통해 특정 가중치들만 살아남게되기 때문에 Sparse Model에 적합하다

### L2 Regularization
- Loss Function 수식에 Weight의 제곱을 더하는 수식의 형태로 Weight에 대한 편미분 수행 시 Weight 자체 값이 줄어드는 형태로 학습이 진행된다
- 이를 통해 특정 Weight가 비이상적으로 커져 학습에 영향을 미치는것을 방지한다
